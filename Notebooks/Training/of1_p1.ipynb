{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac0c89d",
   "metadata": {},
   "source": [
    "# CD4_P1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae996f",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed83b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import layers, models # type: error\n",
    "from tensorflow.keras.applications import VGG16 # type: error\n",
    "from dotenv import load_dotenv\n",
    "# Local Packages\n",
    "from ml_project_util.path import path_definition\n",
    "from ml_project_util.train import train, freeze_layers, unfreeze_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb11524",
   "metadata": {},
   "source": [
    "### Variable Paths, Execution Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a756063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH, PATH_DATASET, PATH_RAWDATA, PATH_JOINEDDATA, PATH_SAVEDMODELS = path_definition()\n",
    "modelname = 'CD4_P1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b812b83",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ddfda8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- block1_conv1 (Conv2D), Trainable: False\n",
      "- block1_conv2 (Conv2D), Trainable: False\n",
      "- block1_pool (MaxPooling2D), Trainable: False\n",
      "- block2_conv1 (Conv2D), Trainable: False\n",
      "- block2_conv2 (Conv2D), Trainable: False\n",
      "- block2_pool (MaxPooling2D), Trainable: False\n",
      "- block3_conv1 (Conv2D), Trainable: False\n",
      "- block3_conv2 (Conv2D), Trainable: False\n",
      "- block3_conv3 (Conv2D), Trainable: False\n",
      "- block3_pool (MaxPooling2D), Trainable: False\n",
      "- block4_conv1 (Conv2D), Trainable: False\n",
      "- block4_conv2 (Conv2D), Trainable: False\n",
      "- block4_conv3 (Conv2D), Trainable: False\n",
      "- block4_pool (MaxPooling2D), Trainable: False\n",
      "- block5_conv1 (Conv2D), Trainable: False\n",
      "- block5_conv2 (Conv2D), Trainable: False\n",
      "- block5_conv3 (Conv2D), Trainable: False\n",
      "- block5_pool (MaxPooling2D), Trainable: False\n",
      "- global_average_pooling2d (GlobalAveragePooling2D), Trainable: False\n",
      "- dense (Dense), Trainable: False\n",
      "- dropout (Dropout), Trainable: False\n",
      "- dense_1 (Dense), Trainable: False\n",
      "- dropout_1 (Dropout), Trainable: False\n",
      "- dense_2 (Dense), Trainable: False\n",
      "\n",
      "\n",
      "- block1_conv1 (Conv2D), Trainable: False\n",
      "- block1_conv2 (Conv2D), Trainable: False\n",
      "- block1_pool (MaxPooling2D), Trainable: False\n",
      "- block2_conv1 (Conv2D), Trainable: False\n",
      "- block2_conv2 (Conv2D), Trainable: False\n",
      "- block2_pool (MaxPooling2D), Trainable: False\n",
      "- block3_conv1 (Conv2D), Trainable: False\n",
      "- block3_conv2 (Conv2D), Trainable: False\n",
      "- block3_conv3 (Conv2D), Trainable: False\n",
      "- block3_pool (MaxPooling2D), Trainable: False\n",
      "- block4_conv1 (Conv2D), Trainable: False\n",
      "- block4_conv2 (Conv2D), Trainable: False\n",
      "- block4_conv3 (Conv2D), Trainable: False\n",
      "- block4_pool (MaxPooling2D), Trainable: False\n",
      "- block5_conv1 (Conv2D), Trainable: False\n",
      "- block5_conv2 (Conv2D), Trainable: False\n",
      "- block5_conv3 (Conv2D), Trainable: False\n",
      "- block5_pool (MaxPooling2D), Trainable: False\n",
      "- global_average_pooling2d (GlobalAveragePooling2D), Trainable: True\n",
      "- dense (Dense), Trainable: True\n",
      "- dropout (Dropout), Trainable: True\n",
      "- dense_1 (Dense), Trainable: True\n",
      "- dropout_1 (Dropout), Trainable: True\n",
      "- dense_2 (Dense), Trainable: True\n",
      "\n",
      "\n",
      "Found 1360 files belonging to 17 classes.\n",
      "Using 1088 files for training.\n",
      "Found 1360 files belonging to 17 classes.\n",
      "Using 272 files for validation.\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.1196 - loss: 0.6609\n",
      "Epoch 1: saving model to C:/Programming_Files/JupyterVSCode/Multiclass_Transfer_Learning/OxfordFlowers_17/SavedModels/OF1/OF1_P1_001_val0.1334.keras\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 4s/step - accuracy: 0.1220 - loss: 0.6531 - val_accuracy: 0.6140 - val_loss: 0.1334\n"
     ]
    }
   ],
   "source": [
    "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = models.Sequential()\n",
    "for layer in vgg_base.layers:\n",
    "    model.add(layer)\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(17, activation='sigmoid'))\n",
    "\n",
    "epochs = 1\n",
    "lr = 1e-3\n",
    "optimizer = 'Adam'\n",
    "load_dotenv()  # loads variables from .env into os.environ\n",
    "platform = os.getenv(\"PLATFORM\")\n",
    "name = 'OF1_P1'\n",
    "\n",
    "freeze_layers(model, verbose=1)\n",
    "unfreeze_head(model, verbose=1)\n",
    "train(model, epochs, lr, optimizer, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febf3adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1360 files belonging to 17 classes.\n",
      "Using 1088 files for training.\n",
      "Found 1360 files belonging to 17 classes.\n",
      "Using 272 files for validation.\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.4571 - loss: 0.1676\n",
      "Epoch 1: saving model to C:/Programming_Files/JupyterVSCode/Multiclass_Transfer_Learning/OxfordFlowers_17/SavedModels/OF1/OF1_P1_001_val0.0665.keras\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 6s/step - accuracy: 0.4592 - loss: 0.1672 - val_accuracy: 0.8125 - val_loss: 0.0665\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, lr, optimizer, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6aaac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1360 files belonging to 17 classes.\n",
      "Using 1088 files for training.\n",
      "Found 1360 files belonging to 17 classes.\n",
      "Using 272 files for validation.\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.6613 - loss: 0.1122\n",
      "Epoch 1: saving model to C:/Programming_Files/JupyterVSCode/Multiclass_Transfer_Learning/OxfordFlowers_17/SavedModels/OF1/OF1_P1_001_val0.0516.keras\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 6s/step - accuracy: 0.6614 - loss: 0.1121 - val_accuracy: 0.8676 - val_loss: 0.0516\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, lr, optimizer, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd3cad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1360 files belonging to 17 classes.\n",
      "Using 1088 files for training.\n",
      "Found 1360 files belonging to 17 classes.\n",
      "Using 272 files for validation.\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.7645 - loss: 0.0805\n",
      "Epoch 1: saving model to C:/Programming_Files/JupyterVSCode/Multiclass_Transfer_Learning/OxfordFlowers_17/SavedModels/OF1/OF1_P1_001_val0.0448.keras\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 6s/step - accuracy: 0.7644 - loss: 0.0805 - val_accuracy: 0.8787 - val_loss: 0.0448\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, lr, optimizer, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae4365c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1360 files belonging to 17 classes.\n",
      "Using 1088 files for training.\n",
      "Found 1360 files belonging to 17 classes.\n",
      "Using 272 files for validation.\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.8105 - loss: 0.0695\n",
      "Epoch 1: saving model to C:/Programming_Files/JupyterVSCode/Multiclass_Transfer_Learning/OxfordFlowers_17/SavedModels/OF1/OF1_P1_001_val0.0400.keras\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 6s/step - accuracy: 0.8102 - loss: 0.0696 - val_accuracy: 0.8787 - val_loss: 0.0400\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs, lr, optimizer, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87522613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1360 files belonging to 2 classes.\n",
      "Using 1088 files for training.\n",
      "Found 1360 files belonging to 2 classes.\n",
      "Using 272 files for validation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train(model, epochs, lr, optimizer, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\ml_project_util\\train.py:34\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, epochs, lr, optimizer, name, parent_name)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:774\u001b[39m, in \u001b[36mbinary_crossentropy\u001b[39m\u001b[34m(target, output, from_logits)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target.shape, output.shape):\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 != e2:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    775\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    776\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mReceived: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    777\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    778\u001b[39m         )\n\u001b[32m    780\u001b[39m output, from_logits = _get_logits(\n\u001b[32m    781\u001b[39m     output, from_logits, \u001b[33m\"\u001b[39m\u001b[33mSigmoid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbinary_crossentropy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    782\u001b[39m )\n\u001b[32m    784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[31mValueError\u001b[39m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 17)"
     ]
    }
   ],
   "source": [
    "train(model, epochs, lr, optimizer, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b3676",
   "metadata": {},
   "source": [
    "### Load Chosen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c988f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Programming_Files\\\\JupyterVSCode\\\\Binary_Classification_Transfer_Learning\\\\CatsDogs\\\\StarredModels\\\\CD1_P1_head_epoch002_val0.0480.keras'\n",
    "model = tf.keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834969d5",
   "metadata": {},
   "source": [
    "### Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{modelname}_continue'\n",
    "checkpoint_path = f\"{SavedModelsPath}\\\\{name}_{{epoch:03d}}_val{{val_loss:.4f}}.keras\"\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_freq='epoch',              # Save every epoch\n",
    "    save_weights_only=False,\n",
    "    save_best_only=False,           # Save every time, not just best\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7967fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "name = f'{modelname}_continue'\n",
    "filepath = f\"{TrainingHistoryPath}\\\\{name}.json\"\n",
    "with open(filepath, 'w') as f:\n",
    "    json.dump(history2.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eb6eb",
   "metadata": {},
   "source": [
    "### Continue Training (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2964d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "name = f'{modelname}_continue2'\n",
    "filepath = f\"{TrainingHistoryPath}\\\\{name}.json\"\n",
    "with open(filepath, 'w') as f:\n",
    "    json.dump(history3.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvPy3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
